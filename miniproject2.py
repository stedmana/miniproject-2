# -*- coding: utf-8 -*-
"""MiniProject2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N3xY1HvG9n-Cvpk7K_97u7gFypyH7h06

# MiniProject 2

General Imports
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import normalize
from sklearn import tree
from sklearn import svm
from sklearn.model_selection import GridSearchCV

import nltk
from nltk.stem import WordNetLemmatizer, PorterStemmer
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.tokenize import word_tokenize
 
stemmer = PorterStemmer()
lemmatiser = WordNetLemmatizer()

import re
nltk.download('stopwords')


from wordcloud import WordCloud, STOPWORDS

"""# Data Processing

Processing of data

### Importing Data
"""

#reading in data and then dropping unnamed axis for training set 

# train = 'https://raw.githubusercontent.com/stedmana/miniproject-2/master/reddit_train.csv'
train = 'reddit_train.csv'
test = 'reddit_test.csv'
df_train = pd.read_csv(train).drop(columns='id')

#reading in data and then dropping unamed axis for test set

# test = 'https://raw.githubusercontent.com/stedmana/miniproject-2/master/reddit_test.csv'
df_test = pd.read_csv(test).drop(columns='id')

# checking if data was imported correctly 

df_train.head()

"""### Cleaning Data/Basic Feature Engineering"""

# check shape
df_train.shape

# remove duplicate posts
df_train.drop_duplicates(keep='first', inplace=True)

# check shape
df_train.shape

# stemming, lemmatizing, and removing stop words in text

#take comments section apply tokenizer, stemmer and stop words
tokenizer = RegexpTokenizer(r'\w+')
stemmer = PorterStemmer() 
stop_words = set(stopwords.words('english'))

strdata = df_train.comments.values.tolist() #convert values to list of lists (each row becomes a separate list)
tokens = [tokenizer.tokenize(str(i)) for i in strdata] #tokenize words in lists
cleaned_list = []
for m in tokens:
    stopped = [i for i in m if str(i).lower() not in stop_words] #remove stop words
    stemmed = [stemmer.stem(i) for i in stopped] #stem words
    cleaned_list.append(stemmed) #append stemmed words to list

backtodf = pd.DataFrame(cleaned_list) #convert list back to pandas dataframe
remove_NaN = backtodf.replace(np.nan, '', regex=True) #remove None (which return as words (str))
mergeddf = remove_NaN.astype(str).apply(lambda x: ' '.join(x), axis=1) #convert cells to strings, merge columns

#merge the pre-processed text with the subreddit names 
train_list = pd.DataFrame(zip(mergeddf, df_train.subreddits))

# re-add columns to our dataset 
train_list.columns = ["comments", "subreddits"]

def make_dict_list():
    l = ['hockey', 'nba', 'leagueoflegends', 'soccer', 'funny', 'movies', 'anime', 'Overwatch', 'trees', 'GlobalOffensive', 'nfl', 'AskReddit', 'gameofthrones', 'conspiracy', 'worldnews', 'wow', 'europe', 'canada', 'Music', 'baseball']
    dict = {}
    for l_i in l:
        dict[l_i] = []
    return dict

def subreddits_list_dict(train_list_in):
    # print(train_list_in)
    dl = make_dict_list()

    for x in train_list_in.iterrows():
        print(x[0])
        dl[x[1]].append(x[0])
    # print(len(dl['hockey']))
    return dl

distribution = subreddits_list_dict(train_list)
frequency_dist = nltk.FreqDist(train_list)
sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)[0:50]

"""Wordmap to utilize and play around with to augment feature engineering choices"""

text = ' '
stopwords = set(STOPWORDS) 
  
# iterate through the words in the comments
for x in df_train.comments[2:10]: 
      
    # typecaste each val to string 
    x = str(x) 
  
    # split the value 
    values = x.split() 
      
    # Converts each token into lowercase 
    for i in range(len(values)): 
        values[i] = values[i].lower() 
          
    for words in values: 
        text = text + words + ' '
  
  
wc = WordCloud(max_words= 100,
                      width = 740, 
                      height = 540,
                      background_color ='black',
                      stopwords=stopwords, 
                      contour_width=3, 
                      min_font_size = 10, colormap="Blues").generate(text) 
  
# plot the WordCloud image                        
plt.figure(figsize = (14, 14)) 
plt.imshow(wc) 
plt.axis("off")

#creation of feature list vs targets 

X = train_list.comments
y = train_list.subreddits

# train test split method to process data 

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, train_size=0.8, test_size=0.2)

"""### **Implementation Comparison with Data**

#### Vectorize Text
"""



#count vectorization - remove stopwords and strip ascii accents
vectorizer = CountVectorizer(stop_words='english', strip_accents = 'ascii', min_df=1)
print(vectorizer)

# vectorize dataset
vectors_train = vectorizer.fit_transform(X_train)
vectors_test = vectorizer.transform(X_test)

clf = LogisticRegression()

print(clf)

"""#### TF-IDF"""

# term weighting for textual features 

tf_idf_vectorizer = TfidfVectorizer(stop_words='english', strip_accents = 'ascii', sublinear_tf=True)
vectors_train_idf = tf_idf_vectorizer.fit_transform(X_train)
vectors_test_idf = tf_idf_vectorizer.transform(X_test)

"""#### Normalization"""

#normalize vectors from IDF (had a higher score then the vectorization of text dataset)

vectors_train_normalized = normalize(vectors_train_idf)
vectors_test_normalized = normalize(vectors_test_idf)

"""#### Data Visualization & Feature Engineering"""

#data processing #2: count the number of words in each post 
def textProcessing2 (text):
  splt = text.split(' ')
  return len(splt)

#takes and processes a list of each post
def processingLoop2(X):
  Y = []
  # print(f'{X[0]}')
  for x_i in X:
    new_slot = [x_i, textProcessing2(x_i)]
    Y.append(new_slot)
  # print(Y)
  return Y

x_train_add_2 = processingLoop2(X_train)

x_test_add_2 = processingLoop2(X_test)

# Visualization of Data - specifically subreddits and their relative count 

plt.figure(figsize=(10, 10))
ax = sns.countplot(x='subreddits',data=df_train)
ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha="right")

"""# Part 2

## Logistic Regression
"""

# Implementation of TF-IDF
clf = LogisticRegression()
clf.fit(vectors_train_normalized, y_train)

y_pred_idf = clf.predict(vectors_test_normalized)

metrics.accuracy_score(y_test, y_pred_idf)

"""## Decision Tree"""

clfD = tree.DecisionTreeClassifier()
clfD = clfD.fit(vectors_train, y_train)

y_pred_normalizedD = clfD.predict(X_test)

metrics.accuracy_score(y_test, y_pred_normalizedD)

"""## Linear Support Vector Classification"""

clfSVC = svm.LinearSVC()

clfSVC.fit(vectors_train_normalized, y_train)

y_pred_SVC = clfSVC.predict(vectors_test_normalized)

metrics.accuracy_score(y_test, y_pred_SVC)

"""# Part 3"""

import csv


def csv_to_list(file_name):
    list_out = []
    cols = 0
    with open(file_name) as csv_to_read:
        csv_reader = csv.reader(csv_to_read, delimiter=',')
        line_count = 0
        for row in csv_reader:
            if line_count == 0:
                print(f'Column names for {file_name} are {", ".join(row)}')
                cols = len(row)
                line_count += 1
            else:
                # if len(row) == cols:
                print
                list_out.append(row)
                line_count += 1
        print(f'Processed {len(list_out)} lines.')
    # list_out = remove_quotes(list_out)
    return list_out

# def remove_quotes(list_in):
#     for l_item in list_in:
#         l_item[1] = l_item[1][:]
#     return list_in


def pipeline_main():
    train = csv_to_list('reddit_train.csv')
    tr = len(train)
    test = csv_to_list('reddit_test.csv')
    ts = len(test)
    sets = k_folds(5, train)
    # for x in sets:
    #     print('part 1: {}   part 2: {}'.format(len(x[0]), len(x[1])))
    print(f'train: {tr}, test: {ts}')
    for set in sets:
        validator_pipeline(set, tf_idf_vectorizer.fit_transform, metrics.accuracy_score)
    # validator_pipeline(train, )
    # trainer(train, print)


def train_stub(train_on):
    # train_on is a list. [0] is the comment, [1] is the subreddit
    pass


def eval_stub(comment_in):
    return 'hockey'


def k_folds(k, list_in):
    list_out = []
    for i_k in range(k):
        leave_in = []
        leave_out = []
        for index, item in enumerate(list_in):
            if index % k == i_k:
                leave_out.append(item)
            else:
                leave_in.append(item)
        list_out.append([leave_in.copy(), leave_out.copy()])
    return list_out


def validator_pipeline(training_list, train_fn, eval_fn):
    training = training_list[0]
    testing = training_list[1]
    
    trainer(training, train_fn)
    class_options = ['hockey', 'nba', 'leagueoflegends',
                     'soccer', 'funny', 'movies', 'anime',
                     'Overwatch', 'trees', 'GlobalOffensive',
                     'nfl', 'AskReddit', 'gameofthrones', 'conspiracy',
                     'worldnews', 'wow', 'europe', 'canada', 'Music', 'baseball']
    true_positive = make_dict()
    true_negative = make_dict()
    false_positive = make_dict()
    false_negative = make_dict()
    accuracy = make_dict_float()
    precision = make_dict_float()
    recall = make_dict_float()
    false_positive_rate = make_dict_float()
    f1_measure = make_dict_float()
    # print(true_positive)
    print(f'{len(training)}   {len(testing)}')
    for test_item in testing:
        # print(test_item)
        result = eval_fn(test_item[1])
        expected = test_item[2]
        list_less_result = list_remove_item(class_options, result)
        list_less_expected = list_remove_item(class_options, expected)
        if result == expected:
            for class_item in list_less_expected:
                true_negative[class_item] += 1
            true_positive[result] += 1
        else:
            false_positive[result] += 1
            false_negative[expected] += 1
            list_less_result_expected = list_remove_item(list_less_result, expected)
            for class_item in list_less_result_expected:
                true_negative[class_item] += 1
    # next part computes accuracy, precision, recall... for all classes
    for class_item in class_options:
        tp = true_positive[class_item]
        tn = true_negative[class_item]
        fp = false_positive[class_item]
        fn = false_negative[class_item]
        x = tp + tn + fp + fn
        # print(f'sum : {x}    expected: {len(testing)}')
        # accuracy[class_item] = (tp + tn) / (tp + fp + tn + fn)  # accuracy = (TP + TN) / (TP + FP + TN + FN)
        # precision[class_item] = tp / (tp + fp)  # Precision = TP / (TP + FP)
        # recall[class_item] = tp / (tp + fn)  # recall = TP / (TP + FN)
        # false_positive_rate[class_item] = fp / (fp + tn)  # false positive rate = FP / (FP + TN)
        # f1_measure[class_item] = \
        #     (2 * precision[class_item] * recall[class_item]) / (precision[class_item] + recall[class_item])
        # F1 measure = 2 * (precision * recall) / (precision + recall)



def list_remove_item(list_in, to_remove):
    list_out = []
    for i in list_in:
        if i != to_remove:
            list_out.append(i)
    return list_out


def make_dict_float():
    l = ['hockey', 'nba', 'leagueoflegends', 'soccer', 'funny', 'movies', 'anime', 'Overwatch', 'trees', 'GlobalOffensive', 'nfl', 'AskReddit', 'gameofthrones', 'conspiracy', 'worldnews', 'wow', 'europe', 'canada', 'Music', 'baseball']
    dict = {}
    for l_i in l:
        dict[l_i] = 0.0
    return dict


def make_dict():
    l = ['hockey', 'nba', 'leagueoflegends', 'soccer', 'funny', 'movies', 'anime', 'Overwatch', 'trees', 'GlobalOffensive', 'nfl', 'AskReddit', 'gameofthrones', 'conspiracy', 'worldnews', 'wow', 'europe', 'canada', 'Music', 'baseball']
    dict = {}
    for l_i in l:
        dict[l_i] = 0
    return dict
    # for i in list_in:
    #     is_there = False
    #     for j in l:
    #         if j == i[2]:
    #             is_there = True
    #     if not is_there:
    #         l.append(i[2])

def trainer(list_in, train_fn):
    for element in list_in:
        train_fn(element[1:])


pipeline_main()

"""# Naive Bayes"""

class NaiveBayes:
    #we pass it the various classes (aka the subreddits)
    def __init__(self, classes):
        self.classes = classes
        
    #data processing
    def cleanstring(self, str_arg):
        clean_str=re.sub('[^a-z\s]+',' ',str_arg,flags=re.IGNORECASE) #every char except alphabets is replaced
        clean_str=re.sub('(\s+)',' ',clean_str) #multiple spaces are replaced by single space
        clean_str=clean_str.lower() #converting the cleaned string to lower case
    
        return clean_str # returning the preprocessed string
    
    #This function goes through one piece of data (one comment) and ticks up each word it contains.
    #Each word only counts once even if repeated. End result if iterated over every comment is a total list of how many comments in each category contain any given word.
    def addToDict(self, item, cat_index):
        words = [] #empty list
        if isinstance(item,np.ndarray): item=item[0]
        for word in item.split():
            if word not in set(stopwords.words('english')):
                if word not in words:
                    self.dicts[cat_index][word]+=1
                    words.append(word)
                    #also add to dictionary of the entire dataset
                    self.allwords[word]+=1
                
    def train(self, data, labels):
        self.examples = data
        self.labels = labels
        if not isinstance(self.examples,np.ndarray): 
            self.examples=np.array(self.examples)
        if not isinstance(self.labels,np.ndarray): 
            self.labels=np.array(self.labels)
        self.dicts = np.array([defaultdict(lambda:0) for index in range(self.classes.shape[0])]) #this array of dictionaries will be used to count word occurances in each class
        #note these two aren't the same things: allwords is a dictionary of the entirety dataset, wordslist is a list of each word that occurs at least once int he data set
        self.allwords = defaultdict(lambda:0)
        self.wordslist = set()
        #these arrays will be used to store the probabilities values for each word in each category
        #note the lambda values of 0 : these just serve as a marker that the word never occurs.
        self.thetaone = np.array([defaultdict(lambda: 0 ) for index in range(self.classes.shape[0])]) #this array will be used to store the probability theta(j1) for each word in each category
        self.thetazero = np.array([defaultdict(lambda: 0) for index in range(self.classes.shape[0])])
        
        #for each subreddit, we go through and count the number of occurances of each word
        for index, category in enumerate(self.classes):
            category_examples = self.examples[labels == category]
            clean_examples = [self.cleanstring(ex) for ex in category_examples]
            clean_examples=pd.DataFrame(data=clean_examples)
            
            np.apply_along_axis(self.addToDict, 1, clean_examples, cat_index=index) 
            print(str(index))
            
        #now we can calculate the target probabilities
        
        self.Pc = np.empty(self.classes.shape[0])
        #default values for words not found
        self.thetaonedef = np.empty(self.classes.shape[0])
        self.thetazerodef = np.empty(self.classes.shape[0])
        #again, for each class
        for index, cat in enumerate(self.classes):            
            self.Pc[index] = np.sum(self.labels == cat) /float(self.labels.shape[0]) #probability of the class occuring
            words = set(self.dicts[index].keys()) #get each word
            self.wordslist.update(words)
            for word in words:
                self.thetaone[index][word] = (self.dicts[index][word] + 1) / float(np.sum(self.labels == cat) + 2) #number of comments in this class where the word occurs over the number of comments in this class
                self.thetazero[index][word] = (self.allwords[word] - self.dicts[index][word] +1) / float(np.sum(self.labels != cat) +2) #number of comments NOT in this class where the word occurs over number of commends not in this class
            
            #default values if the tested input includes a word that never occurs
            self.thetaonedef[index] = 1 / float(np.sum(self.labels == cat) + 2)
            self.thetazerodef[index] = 1 / float(np.sum(self.labels != cat) + 2)
            print(str(index))
    
    #returns the probability for the value to be in each class.
    def testValue(self, test_example):
        delta  = np.zeros(self.classes.shape[0]) #this will be used to store the probabilities and returned at the end
        inputwords = defaultdict(lambda:0)
        
        #first clean the input
        inputstring = self.cleanstring(test_example)
        for word in inputstring.split():
            if word not in set(stopwords.words('english')):
                inputwords[word] = 1
        
        for index, cat in enumerate(self.classes):
            delta[index] += math.log(self.Pc[index] / (1 - self.Pc[index]))
            for word in self.wordslist:
                t1 = self.thetaone[index][word]
                if t1 == 0:
                    t1 = self.thetaonedef[index]
                t0 = self.thetazero[index][word]
                if t0 == 0:
                    t0 = self.thetazerodef[index]
                if inputwords[word] == 1:
                    total = math.log(t1 / t0)
                else:
                    total = math.log((1 - t1) / (1 - t0))                
                delta[index] += total
        
        return delta
    
    def test(self, test_set):
        probabilities = []
        
        for example in test_set:
            prob = self.testValue(example)
            probabilities.append(self.classes[np.argmax(prob)])
            #print(example + " | " + self.classes[np.argmax(prob)])
        return probabilities